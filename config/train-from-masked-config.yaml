defaults:
  - train-lm
  - model: from_masked_config
  - collator: masked_modeling
  - _self_
model:
  config:
      # it would be nice to infer this automatically from a tokenizer vocab, but doing so
      # would require instantiating the tokenizer, which is not possible in the config
      # in other words, would lose the maximal dependency injection that we currently have
      vocab_size: 50002
