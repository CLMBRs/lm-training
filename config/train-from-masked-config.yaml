defaults:
  - train-lm
  - model: from_masked_config
  - collator: masked_modeling
  - _self_

model:
  config:
      # it would be nice to infer this automatically from a tokenizer vocab, but doing so
      # would require instantiating the tokenizer, which is not possible in the config
      # in other words, would lose the maximal dependency injection that we currently have
      vocab_size: 50002

# Override the tokenizer configuration from train-lm.yaml
tokenizer:
  tokenizer_file: models/tokenizer/word-level-masked.json  # New tokenizer file path
  unk_token: "<unk>"   # New unknown token, if you need to change it
  eos_token: "<eos>"   # New end-of-sequence token, if you need to change it
  pad_token: "<pad>"  # New padding token, if you need to change it
  mask_token: "<MASK>"