defaults:
  - train-from-config
  - override model/config: lstm
  - _self_
model:
  config:
      # it would be nice to infer this automatically from a tokenizer vocab, but doing so
      # would require instantiating the tokenizer, which is not possible in the config
      # in other words, would lose the maximal dependency injection that we currently have
      vocab_size: 50002
      embedding_dim: 1024
      hidden_dim: 1024
      num_layers: 2
      dropout_p: 0.1
      tie_weights: True
