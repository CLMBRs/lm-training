defaults:
  - data: raw
  - dataset: load_dataset_text
  - tokenizer: from-file
  - training_args: base
  - _self_
seed: 42
data:
  base_dir: ../data/wiki
# dataset:
#   data_files: ${data.splits}
tokenizer:
  tokenizer_file: models/tokenizer/word-level.json
  # these first two special tokens come from the data
  unk_token: "<unk>"
  eos_token: "<eos>"
  # pad token comes from how we trained the tokenizer; see tokenizer/word-level.yaml
  pad_token: "<pad>"
  # NB: we could set model_max_length via interpolation (e.g. ${model.config.max_position_embeddings}),
  # but not every `model` will have that path necessarily
  model_max_length: 512
text_field: "text"
train_split: "train"
eval_split: "valid"
model: ???
training_args:
  seed: ${seed}
  save_steps: 10
  eval_steps: 10
  logging_steps: 10
  log_level: info
  dataloader_num_workers: 8
  fp16: true
use_iterable_dataset: true
train_shards_per_worker: 32
eval_shards_per_worker: 8
trainer:
  _target_: transformers.Trainer
  args: ${training_args}
  model: ${model}
  tokenizer: ${tokenizer}
  callbacks:
    - _target_: transformers.EarlyStoppingCallback
      early_stopping_patience: 5
